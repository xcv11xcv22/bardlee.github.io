---
tags:
  - pytorch
title: multiprocessing vs pytorch
created: 2025-03-26T16:00:00
modified: 2025-03-26T16:00:00
---


åœ¨ PyTorch ä¸­é€²è¡Œæ¨¡å‹è¨“ç·´æˆ–æ¨è«–æ™‚ï¼Œ**GPU ä½¿ç”¨ç‡å·²ç¶“å¾ˆé«˜ï¼Œç”šè‡³ CPU ä¹Ÿåé«˜**ï¼Œé€™æ˜¯å› ç‚º **PyTorch èƒŒå¾Œå·²ç¶“å…§å»ºäº†é«˜åº¦å„ªåŒ–çš„ä¸¦è¡Œè™•ç†æ©Ÿåˆ¶**ï¼ŒåŒ…æ‹¬ï¼š

- **GPU è¨ˆç®—ï¼šä½¿ç”¨ CUDA é€²è¡ŒçŸ©é™£é‹ç®—ï¼ˆTensorCoreã€cuDNNã€cuBLAS ç­‰ï¼‰**
    
- **CPU è³‡æºï¼šç”¨æ–¼è³‡æ–™åŠ è¼‰ã€å‰å¾Œè™•ç†ã€æ¢¯åº¦é‹ç®—ç­‰**
    
- **è³‡æ–™åŠ è¼‰ï¼ˆDataLoaderï¼‰ï¼šå¯å…§å»ºä½¿ç”¨å¤šé€²ç¨‹ `multiprocessing` åŠ é€Ÿ I/O**


## âœ… PyTorch æœ¬èº«æ˜¯å¦éœ€è¦æ‰‹å‹•ä½¿ç”¨ `multiprocessing`ï¼Ÿ

åœ¨å¤§å¤šæ•¸æƒ…æ³ä¸‹ï¼Œ**ä¸éœ€è¦è‡ªå·±å¯« `multiprocessing.Process()` ä¾†è·‘ PyTorch**ï¼Œå› ç‚ºï¼š

- PyTorch æœ¬èº«çš„è³‡æ–™åŠ è¼‰ã€æ¨¡å‹è¨“ç·´å°±å·²ç¶“æ”¯æ´ä¸¦è¡Œ
    
- ä½¿ç”¨ `torch.utils.data.DataLoader` æ™‚ï¼Œå¯ä»¥ç›´æ¥é€é `num_workers` ä¾†è‡ªå‹•é–‹å¤šå€‹å­é€²ç¨‹é€²è¡Œè³‡æ–™åŠ è¼‰
    
- å¤š GPU è¨“ç·´æ™‚ï¼Œæ¨è–¦ä½¿ç”¨å…§å»ºçš„åˆ†æ•£å¼è¨“ç·´ APIï¼Œå¦‚ `torch.nn.DataParallel` æˆ– `torch.distributed`
---



> #### num_workers > 0 æ™‚ï¼ŒèƒŒæ™¯è‡ªå‹•å•Ÿå‹•multiprocessing

## ä½¿ç”¨ PyTorch æ­é… `multiprocessing` æ™‚çš„æ³¨æ„äº‹é …

|å•é¡Œ|å»ºè­°|
|---|---|
|CUDA åˆå§‹åŒ–æœƒåœ¨ fork å¾Œå¤±æ•—|é¿å…åœ¨ `multiprocessing` ä¸­ fork å¾Œæ‰ä½¿ç”¨ CUDA|
|å¤š GPU æ¨¡å‹è¨“ç·´|å»ºè­°ä½¿ç”¨ `torch.distributed` è€Œéæ‰‹å‹• `multiprocessing`|
|CUDA Tensor åœ¨å¤šé€²ç¨‹ä¸­å‚³é|**ä¸è¦åœ¨é€²ç¨‹é–“å‚³é CUDA Tensor**ï¼Œå®¹æ˜“å‡ºéŒ¯ï¼Œè«‹åœ¨æ¯å€‹é€²ç¨‹å…§ç¨ç«‹å»ºç«‹ GPU å¼µé‡|
|å¤šé€²ç¨‹è³‡æ–™åŠ è¼‰æ­»é–|ç¢ºä¿ç¨‹å¼åœ¨ `__main__` ä¸‹åŸ·è¡Œï¼Œä¸” `DataLoader` çš„ `num_workers` ä¸è¦è¨­éé«˜|

---

## âœ… çµè«–èˆ‡å»ºè­°

| éœ€æ±‚                      | å»ºè­°                                                        |
| ----------------------- | --------------------------------------------------------- |
| **å–® GPU è¨“ç·´ / æ¨è«–**ï¼Œæƒ³æå‡æ•ˆèƒ½ | âœ… ä½¿ç”¨ `DataLoader(num_workers > 0)`                        |
| **å¤š GPU ä¸¦è¡Œä»»å‹™ï¼ˆéåŒæ­¥è¨“ç·´ï¼‰**   | âœ… è‡ªè¡Œç”¨ `multiprocessing.Process` åˆ†åˆ¥è·‘                       |
| **å¤š GPU è¨“ç·´åŒä¸€æ¨¡å‹**        | âœ… ä½¿ç”¨ `torch.nn.DataParallel` æˆ– `torch.distributed.launch` |
| **CPU åˆ©ç”¨ç‡éé«˜**           | âœ… æª¢æŸ¥ `num_workers`ã€`pin_memory` è¨­å®šï¼Œæˆ–è€ƒæ…®è³‡æ–™å‰è™•ç†æ˜¯å¦éé‡           |
| **GPU ä½¿ç”¨ç‡å·²æ»¿ä½†é€Ÿåº¦ä»æ…¢**      | âœ… æª¢æŸ¥è³‡æ–™è¼‰å…¥ç“¶é ¸ï¼Œå¯ä½¿ç”¨é è®€ï¼ˆ`prefetch_factor`ï¼‰æˆ– LMDB å„²å­˜è³‡æ–™            |


## é¡å¤–èªªæ˜

## ä¸€ã€**ã€ŒCUDA åˆå§‹åŒ–æœƒåœ¨ fork å¾Œå¤±æ•—ã€æ˜¯ä»€éº¼æ„æ€ï¼Ÿ**

### ğŸ”¸èƒŒæ™¯

- åœ¨ Linuxï¼ˆåŒ…å« Fedoraï¼‰ä¸­ï¼Œ`multiprocessing` çš„é è¨­é€²ç¨‹å•Ÿå‹•æ–¹å¼æ˜¯ **fork**ã€‚
    
- ã€Œforkã€æœƒ**è¤‡è£½çˆ¶é€²ç¨‹çš„è¨˜æ†¶é«”ç‹€æ…‹**ï¼ˆé€£åŒ CUDA ä¸Šçš„ä¸Šä¸‹æ–‡ï¼‰ï¼Œä½† CUDA ä¸¦**ä¸æ”¯æ´é€™ç¨® fork å¾Œçš„ä¸Šä¸‹æ–‡ç¹¼æ‰¿**ã€‚
    

### ğŸ§¨ çµæœæœƒæ€æ¨£ï¼Ÿ

å¦‚æœä½ åœ¨ä¸»é€²ç¨‹ä¸­åˆå§‹åŒ–äº† CUDAï¼ˆä¾‹å¦‚å‘¼å« `torch.cuda.is_available()`ã€å»ºç«‹ tensorï¼‰ç„¶å¾Œå†ç”¨ `Process()` fork å­é€²ç¨‹ï¼Œ**å­é€²ç¨‹ä¸­å†ä½¿ç”¨ CUDA æ™‚å¯èƒ½æœƒå ±éŒ¯**ã€‚

### ğŸš« éŒ¯èª¤ç¤ºç¯„ï¼š



# ä¸»é€²ç¨‹å…ˆåˆå§‹åŒ–äº† CUDA

```python
import torch
from multiprocessing import Process
torch.cuda.is_available()  # åˆå§‹åŒ– CUDA

def worker():
    # åœ¨å­é€²ç¨‹ä¸­ä½¿ç”¨ GPU
    x = torch.tensor([1.0]).cuda()  # å¯èƒ½å ±éŒ¯ï¼
    print(x)

if __name__ == '__main__':
    p = Process(target=worker)
    p.start()
    p.join()

â—å¸¸è¦‹éŒ¯èª¤ï¼š

RuntimeError: CUDA error: initialization error
```



âœ… æ­£ç¢ºåšæ³•ï¼š

ğŸ‘‰ é¿å…åœ¨ fork å­é€²ç¨‹ä¹‹å‰åˆå§‹åŒ– CUDAï¼
```python
import torch
from multiprocessing import Process

def worker():
    # åœ¨å­é€²ç¨‹ä¸­å†åˆå§‹åŒ– CUDA
    x = torch.tensor([1.0]).cuda()  # OKï¼
    print(x)

if __name__ == '__main__':
    p = Process(target=worker)
    p.start()
    p.join()
```


## äºŒã€ã€Œä¸è¦åœ¨é€²ç¨‹é–“å‚³é CUDA Tensorã€æ˜¯ä»€éº¼æ„æ€ï¼Ÿ
ğŸ”¸èƒŒæ™¯

    multiprocessing çš„è³‡æ–™äº¤æ›æ˜¯é€é åºåˆ—åŒ–ï¼ˆpickleï¼‰ å‚³éç‰©ä»¶ã€‚

    ä½† CUDA Tensor ä¸æ˜¯æ™®é€šçš„è¨˜æ†¶é«”è³‡æ–™ï¼Œå®ƒå­˜åœ¨ GPU çš„é¡¯å­˜ä¸­ï¼ŒPython çš„åºåˆ—åŒ–ä¸¦ä¸èƒ½è™•ç†å®ƒã€‚

ğŸ§¨ é€™æ¨£æœƒå ±éŒ¯æˆ–è¡Œç‚ºä¸æ­£ç¢ºï¼š
```python
import torch
from multiprocessing import Process, Queue

def worker(q):
    t = q.get()         # å˜—è©¦å¾ queue ä¸­æ‹¿ CUDA Tensor
    print(t.cuda())     # å¯èƒ½éŒ¯èª¤æˆ–è¡Œç‚ºä¸é æœŸ

if __name__ == '__main__':
    q = Queue()
    x = torch.tensor([1.0]).cuda()
    q.put(x)            # å˜—è©¦å‚³ CUDA Tensorï¼ˆä¸è¡ŒâŒï¼‰

    p = Process(target=worker, args=(q,))
    p.start()
    p.join()
```


âœ… æ­£ç¢ºåšæ³•ï¼š

ä½ å¯ä»¥ åªå‚³ CPU Tensorã€numpyã€intã€float ç­‰æ™®é€šè³‡æ–™ï¼Œå†è®“å­é€²ç¨‹è‡ªå·±æ¬ä¸Š GPUã€‚

```python
import torch
from multiprocessing import Process, Queue

def worker(q):
    x_cpu = q.get()            # æ‹¿åˆ° CPU Tensor
    x_gpu = x_cpu.cuda()       # åœ¨å­é€²ç¨‹è‡ªå·±æ¬ä¸Š GPU
    print(x_gpu)

if __name__ == '__main__':
    q = Queue()
    x = torch.tensor([1.0])    # CPU Tensor
    q.put(x)

    p = Process(target=worker, args=(q,))
    p.start()
    p.join()
```

|å•é¡Œ|éŒ¯èª¤|è§£æ³•|
|---|---|---|
|fork å¾Œ CUDA åˆå§‹åŒ–å¤±æ•—|å­é€²ç¨‹ç”¨åˆ° CUDA æ™‚å ±éŒ¯|â—ä¸è¦åœ¨ä¸»é€²ç¨‹ç”¨åˆ° CUDAï¼Œè®“å­é€²ç¨‹è‡ªå·±åˆå§‹åŒ–|
|å¤šé€²ç¨‹å‚³ CUDA Tensor|å¯èƒ½ç„¡æ³•åºåˆ—åŒ– / æå£|âœ… å‚³ CPU Tensorï¼Œå­é€²ç¨‹è‡ªå·±è½‰ `.cuda()`|

---
